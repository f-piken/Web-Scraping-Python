# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10NeiLEn97SSfUY8VxTVpj8wI7qpv1AZA
"""

import requests
import pandas as pd
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup

def scrape_page(url):

    UserAgent = Request(url, headers={'User-Agent': 'Mozilla/5.0'})

    try:
        html_text = urlopen(UserAgent)
    except Exception as e:
        print(f"An error occurred: {e}")
        return pd.DataFrame()

    soup = BeautifulSoup(html_text, "html.parser")

    daftar = soup.find_all('tag',{'class':'nama class'})

    #class boleh di kosongi
    judul = [tag.find_previous('tag',{'class':'nama class'}).text.strip() if tag.find_previous('tag') is not None else None for tag in daftar]
    tanggal = [tag.find_previous('tag',{'class':'nama class'}).text.strip() if tag.find_previous('tag') is not None else None for tag in daftar]

    return pd.DataFrame({
        'Judul': judul,
        'Tanggal': tanggal
    })

base_url = 'link tujuan'

all_data = pd.DataFrame()

page_number = 1

while True:
    print(f'Scraping page {page_number}...')
    current_url = base_url.format(page_number)

    # Scrape the current page
    page_data = scrape_page(current_url)

    # If no data is found, we have reached the last page
    if page_data.empty:
        break

    # Append the data to the main DataFrame
    all_data = pd.concat([all_data, page_data], ignore_index=True)

    # Move to the next page
    page_number += 1

# Print the final DataFrame
print(all_data)

all_data.to_excel('nama file.xlsx', index=False)